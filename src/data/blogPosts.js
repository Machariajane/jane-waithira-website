// src/data/blogPosts.js
const blogPosts = [
    {
        id: "building-shopSmart",
        title: "Building shopSmart: A Conversational Shopping Agent with GenAI & LangGraph",
        date: "April 2025",
        author: "Jane Waithira",
        excerpt: "Learn how to combine a modern LLM (Gemini) with a small Python toolkit and LangGraph orchestration to build a multi-step shopping agent.",
        coverImage: "/images/blog/default.jpg",
        tags: ["GenAI", "LangGraph", "Agents", "Python"],
        content: `
# Building shopSmart: A Conversational Shopping Agent with GenAI & LangGraph

*By Jane Waithira ‚Ä¢ April 2025*

---

## üéØ The Problem

Online shopping across multiple marketplaces can be tedious:

- You want a particular brand, size, color, and price range.  
- You have to jump between Amazon, eBay, and more to compare listings.  
- Manually reading ratings, reviews, delivery estimates‚Ä¶ it all adds up to decision fatigue.

**Goal:** Let a generative AI‚Äìpowered agent understand your natural‚Äëlanguage request and do the heavy lifting: search, filter, compare, and recommend‚Äîautomatically.

---

## ü§ñ How GenAI Helps

Generative AI (e.g. Google's Gemini) excels at:

1. **Natural‚Äëlanguage understanding:** Parse "Show me Nike running shoes under $150, size 9, in black."  
2. **Decision logic:** We can have it choose the best product by price, rating, or delivery time.  
3. **Function calling:** Seamlessly invoke Python tools to run searches or fetch details.  

By combining an LLM with a small toolkit of Python "@tool" functions, we get an **agent** that talks to you like a human assistant but executes code under the hood.

---

## üèóÔ∏è Architecture Overview

1. **Tool Functions**  
   We define simple Python functions for:  
   - \`search_shoes(query, preferences) ‚Üí JSON\`  
   - \`compare_products(product_ids) ‚Üí JSON\`  
   - \`get_product_details(product_id) ‚Üí JSON\`  
   - \`update_preferences(prefs) ‚Üí JSON\`  

   Each is decorated with \`@tool\` so the LLM can call it directly:

   \`\`\`python
   @tool
   def search_shoes(query: str, preferences: Optional[Dict[str,Any]] = None) -> str:
       """
       Filters mock Amazon/eBay catalogs for matching shoes.
       Returns JSON summary of matches.
       """
       # ... filter logic, fallback if no exact matches ...
       return json.dumps(results)
   \`\`\`

2. **Conversation Graph (LangGraph)**  
   We orchestrate the flow via a \`StateGraph\`:

   \`\`\`python
   from langgraph.graph import StateGraph, START, END
   from langgraph.prebuilt import ToolNode

   # Create nodes
   workflow = StateGraph(ShoppingState)
   workflow.add_node("chatbot", chatbot_node)
   workflow.add_node("tools", ToolNode([search_shoes, compare_products, ‚Ä¶]))
   workflow.add_node("ordering", ordering_node)

   # Add edges
   workflow.add_edge(START, "chatbot")
   workflow.add_edge("chatbot", "tools", condition=‚Ä¶)
   workflow.add_edge("tools", "chatbot")
   workflow.add_edge("chatbot", "ordering", condition=‚Ä¶)
   workflow.add_edge("ordering", "chatbot")
   \`\`\`

3. **LLM Binding**  
   We bind Gemini via \`langchain_google_genai\`:

   \`\`\`python
   from langchain_google_genai import ChatGoogleGenerativeAI
   llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash-latest")
   llm_with_tools = llm.bind_tools([search_shoes, compare_products, get_product_details, update_preferences])
   \`\`\`

   Inside our \`chatbot_node\`, prompts combine system instructions with conversation history, and we call:

   \`\`\`python
   response = llm_with_tools.invoke([
       {"role": "system", "content": system_prompt},
       *state["messages"]
   ])
   \`\`\`

---

## ‚öôÔ∏è Example Interaction

\`\`\`
You: Show me Adidas sneakers under $120 in size 9.
Agent: Searching for Adidas shoes under $120 in size 9‚Ä¶
Agent: I found 3 matches on Amazon and 2 on eBay. Here's the top pick:   
   ‚Ä¢ **Amazon ‚Äì Adidas Ultraboost 21** ($110, 4.7‚òÖ, 3‚Äëday delivery)  
Would you like details or compare options?
\`\`\`

Behind the scenes, the model called \`search_shoes\`, parsed the JSON, and generated that summary.

---

## ‚ö†Ô∏è Limitations

1. **Mock Data**  
   Right now we use static Python lists. Real‚Äëworld APIs will require authentication, rate limits, pagination, and error handling.

2. **LLM Dependence & Cost**  
   Each user turn incurs API calls. High chat volume can become expensive and suffer latency.

3. **Edge Cases**  
   - Ambiguous requests ("I want something comfy") need more sophisticated preference elicitation.  
   - Numeric conversions (EU vs. US sizes) are approximate.

---

## üöÄ Future & Art‚Äëof‚Äëthe‚ÄëPossible

- **Real‚ÄëTime Market Data**  
  Integrate Amazon's and eBay's official APIs, plus other retailers.

- **Vector Search & RAG**  
  Use embeddings to index a large product catalog and retrieve relevant items via semantic similarity.

- **Multi‚ÄëModal Inputs**  
  Let users upload images of shoes they like, then use Vision + LLM to find similar products.

- **Persistent Profiles**  
  Store preferences and past orders in a database; use context caching for personalized recommendations over time.

- **Front‚ÄëEnd UI**  
  Build a Streamlit or React app so users can chat with shopSmart in their browser or mobile.

---

## üéâ Conclusion

We've shown how to combine a modern LLM (Gemini) with a small Python toolkit and LangGraph orchestration to build a multi‚Äëstep shopping agent. This pattern generalizes to many domains‚Äîanywhere you need conversational retrieval, filtering, and structured outputs.

 
`
    }
];

export default blogPosts;